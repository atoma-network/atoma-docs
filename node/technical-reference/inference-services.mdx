---
title: 'Inference Services'
description: 'Learn about the inference services available in Atoma Node'
---

## Overview

Atoma Node integrates several leading open-source inference engines:

- **vLLM**: A high-throughput and memory-efficient inference engine optimized for LLMs. Features state-of-the-art serving throughput with PagedAttention memory management and continuous batching.

- **mistral.rs**: A blazingly fast Rust-based inference engine with support for various model architectures, quantization methods, and hardware acceleration options.

- **Text Embeddings Inference (TEI)**: A high-performance solution specifically designed for text embedding models, offering both REST and gRPC APIs with support for various embedding model architectures.

## Chat Completions

| Backend                                                  | Architecture/Platform | Docker Compose Profile           |
| -------------------------------------------------------- | --------------------- | -------------------------------- |
| [vLLM](https://github.com/vllm-project/vllm)             | CUDA                  | `chat_completions_vllm`          |
| [vLLM](https://github.com/vllm-project/vllm)             | x86_64                | `chat_completions_vllm_cpu`      |
| [vLLM](https://github.com/vllm-project/vllm)             | ROCm                  | `chat_completions_vllm_rocm`     |
| [mistral.rs](https://github.com/EricLBuehler/mistral.rs) | x86_64, aarch64       | `chat_completions_mistralrs_cpu` |

## Embeddings

| Backend                                                                               | Architecture/Platform | Docker Compose Profile |
| ------------------------------------------------------------------------------------- | --------------------- | ---------------------- |
| [Text Embeddings Inference](https://github.com/huggingface/text-embeddings-inference) | CUDA                  | `embeddings_tei`       |

## Image Generations

| Backend                                                  | Architecture/Platform | Docker Compose Profile        |
| -------------------------------------------------------- | --------------------- | ----------------------------- |
| [mistral.rs](https://github.com/EricLBuehler/mistral.rs) | CUDA                  | `image_generations_mistralrs` |

To run the node with confidential compute mode, you can use the following command:

```bash
# Build and start all services
COMPOSE_PROFILES=chat_completions_vllm,embeddings_tei,image_generations_mistralrs,confidential docker compose up --build

# Only start one service
COMPOSE_PROFILES=chat_completions_vllm,confidential docker compose up --build

# Run in detached mode
COMPOSE_PROFILES=chat_completions_vllm,embeddings_tei,image_generations_mistralrs,confidential docker compose up -d --build
```

Otherwise, you can run the node in non-confidential mode with:

```bash
# Build and start all services
COMPOSE_PROFILES=chat_completions_vllm,embeddings_tei,image_generations_mistralrs,non-confidential docker compose up --build

# Only start one service
COMPOSE_PROFILES=chat_completions_vllm,non-confidential docker compose up --build

# Run in detached mode
COMPOSE_PROFILES=chat_completions_vllm,embeddings_tei,image_generations_mistralrs,non-confidential docker compose up -d --build
```
