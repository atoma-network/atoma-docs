openapi: 3.1.0
info:
  title: atoma-proxy
  description: ''
  license:
    name: Apache-2.0
  version: 0.1.0
servers:
- url: http://localhost:8080
paths:
  /health:
    get:
      tags:
      - Health
      summary: Health
      operationId: health
      responses:
        '200':
          description: Service is healthy
          content:
            application/json:
              schema: {}
        '500':
          description: Service is unhealthy
  /node/registration:
    post:
      tags:
      - Node Public Address Registration
      summary: Register node
      description: |-
        This endpoint allows nodes to register or update their public address in the system.
        When a node comes online or changes its address, it can use this endpoint to ensure
        the system has its current address for routing requests.

        # Arguments

        * `state` - The shared application state containing the state manager sender
        * `payload` - The registration payload containing the node's ID and public address

        # Returns

        Returns `Ok(Json(Value::Null))` on successful registration, or an error status code
        if the registration fails.

        # Errors

        Returns `StatusCode::INTERNAL_SERVER_ERROR` if:
        * The state manager channel is closed
        * The registration event cannot be sent

        # Example Request Payload

        ```json
        {
            "node_small_id": 123,
            "public_address": "http://node-123.example.com:8080"
        }
        ```
      operationId: node_public_address_registration
      responses:
        '200':
          description: Node public address registered successfully
          content:
            application/json:
              schema: {}
        '500':
          description: Failed to register node public address
  /v1/chat/completions:
    post:
      tags:
      - Chat Completions
      summary: Create chat completion
      description: |-
        This function processes chat completion requests by determining whether to use streaming
        or non-streaming response handling based on the request payload. For streaming requests,
        it configures additional options to track token usage.

        # Arguments

        * `metadata`: Extension containing request metadata (node address, ID, compute units, etc.)
        * `state`: The shared state of the application
        * `headers`: The headers of the request
        * `payload`: The JSON payload containing the chat completion request

        # Returns

        Returns a Response containing either:
        - A streaming SSE connection for real-time completions
        - A single JSON response for non-streaming completions

        # Errors

        Returns an error status code if:
        - The request processing fails
        - The streaming/non-streaming handlers encounter errors
        - The underlying inference service returns an error
      operationId: chat_completions_handler
      requestBody:
        content:
          application/json:
            schema: {}
        required: true
      responses:
        '200':
          description: Chat completions
          content:
            application/json:
              schema: {}
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '500':
          description: Internal server error
  /v1/embeddings:
    post:
      tags:
      - Embeddings
      summary: Create embeddings
      description: |-
        This endpoint follows the OpenAI API format for generating vector embeddings from input text.
        The handler receives pre-processed metadata from middleware and forwards the request to
        the selected node.

        Note: Authentication, node selection, and initial request validation are handled by middleware
        before this handler is called.

        # Arguments
        * `metadata` - Pre-processed request metadata containing node information and compute units
        * `state` - The shared proxy state containing configuration and runtime information
        * `headers` - HTTP headers from the incoming request
        * `payload` - The JSON request body containing the model and input text

        # Returns
        * `Ok(Response)` - The embeddings response from the processing node
        * `Err(StatusCode)` - An error status code if any step fails

        # Errors
        * `INTERNAL_SERVER_ERROR` - Processing or node communication failures
      operationId: embeddings_handler
      requestBody:
        content:
          application/json:
            schema: {}
        required: true
      responses:
        '200':
          description: Embeddings generated successfully
          content:
            application/json:
              schema: {}
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '500':
          description: Internal server error
  /v1/images/generations:
    post:
      tags:
      - Image Generations
      summary: Create image generation
      description: |-
        This endpoint processes requests to generate images using AI models by forwarding them
        to the appropriate AI node. The request metadata and compute units have already been
        validated by middleware before reaching this handler.

        # Arguments
        * `metadata` - Extension containing pre-processed request metadata (node address, compute units, etc.)
        * `state` - Application state containing configuration and shared resources
        * `headers` - HTTP headers from the incoming request
        * `payload` - JSON payload containing image generation parameters

        # Returns
        * `Result<Response<Body>, StatusCode>` - The processed response from the AI node or an error status

        # Errors
        * Returns various status codes based on the underlying `handle_image_generation_response`:
          - `INTERNAL_SERVER_ERROR` - If there's an error communicating with the AI node

        # Example Payload
        ```json
        {
            "model": "stable-diffusion-v1-5",
            "n": 1,
            "size": "1024x1024"
        }
        ```
      operationId: image_generations_handler
      requestBody:
        content:
          application/json:
            schema: {}
        required: true
      responses:
        '200':
          description: Image generations
          content:
            application/json:
              schema: {}
        '400':
          description: Bad request
        '401':
          description: Unauthorized
        '500':
          description: Internal server error
  /v1/models:
    get:
      tags:
      - Models
      summary: List models
      description: |-
        This endpoint mimics the OpenAI models endpoint format, returning a list of
        available models with their associated metadata and permissions. Each model
        includes standard OpenAI-compatible fields to ensure compatibility with
        existing OpenAI client libraries.

        # Arguments

        * `state` - The shared application state containing the list of available models

        # Returns

        Returns a JSON response containing:
        * An "object" field set to "list"
        * A "data" array containing model objects with the following fields:
          - id: The model identifier
          - object: Always set to "model"
          - created: Timestamp (currently hardcoded)
          - owned_by: Set to "atoma"
          - root: Same as the model id
          - parent: Set to null
          - max_model_len: Maximum context length (currently hardcoded to 2048)
          - permission: Array of permission objects describing model capabilities

        # Example Response

        ```json
        {
          "object": "list",
          "data": [
            {
              "id": "meta-llama/Llama-3.1-70B-Instruct",
              "object": "model",
              "created": 1730930595,
              "owned_by": "atoma",
              "root": "meta-llama/Llama-3.1-70B-Instruct",
              "parent": null,
              "max_model_len": 2048,
              "permission": [
                {
                  "id": "modelperm-meta-llama/Llama-3.1-70B-Instruct",
                  "object": "model_permission",
                  "created": 1730930595,
                  "allow_create_engine": false,
                  "allow_sampling": true,
                  "allow_logprobs": true,
                  "allow_search_indices": false,
                  "allow_view": true,
                  "allow_fine_tuning": false,
                  "organization": "*",
                  "group": null,
                  "is_blocking": false
                }
              ]
            }
          ]
        }
        ```
      operationId: models_handler
      responses:
        '200':
          description: List of available models
          content:
            application/json:
              schema: {}
        '500':
          description: Failed to retrieve list of available models
components: {}
tags:
- name: Health
  description: Health check
- name: Chat Completions
  description: Chat completions
- name: Models
  description: Models
- name: Node Public Address Registration
  description: Node public address registration
- name: Embeddings
  description: OpenAI's API embeddings v1 endpoint
- name: Image Generations
  description: OpenAI's API image generations v1 endpoint
