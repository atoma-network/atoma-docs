---
title: Get Started
description: Learn how to use Atoma's Cloud API for AI inference
---

Atoma provides a fully compatible OpenAI API that allows you to access decentralized AI capabilities while maintaining privacy and security. This guide will help you get started with using the API.

## Authentication

All API requests require authentication using a bearer tokenm, you can get your API key from the [Atoma Dashboard](https://cloud.atoma.network). Include your API key in the `Authorization` header:

```bash
Authorization: Bearer <YOUR_API_KEY>
```

## Base URL

All API endpoints are accessible through the base URL:

```
https://api.atoma.network
```

## Available Endpoints

Atoma provides the following main endpoints:

### Standard Endpoints (OpenAI Compatible)
- Chat Completions: `/v1/chat/completions`
- Embeddings: `/v1/embeddings`
- Image Generations: `/v1/images/generations`
- Models: `/v1/models`

### Confidential Computing Endpoints
- Confidential Chat: `/v1/confidential/chat/completions`
- Confidential Embeddings: `/v1/confidential/embeddings`
- Confidential Images: `/v1/confidential/images/generations`

### Nodes
- Node Registration: `/v1/nodes`
- Node Lock Creation: `/v1/nodes/lock`

The nodes endpoints allow for:
- Node registration and public address updates in the network
- Creating node locks for confidential computing
- Managing node public keys for encrypted communication

## Quick Start Example

Here's a simple example using the Python SDK:

```python
from atoma_sdk import AtomaSDK
import os

# Initialize the SDK with your API key
atoma_sdk = AtomaSDK(
    bearer_auth=os.getenv("ATOMASDK_BEARER_AUTH", "")  # Get API key from environment variable
)

# Make a chat completion request
response = atoma_sdk.chat.create(
    model="meta-llama/llama-3.3-70b-instruct",  # Specify the model you want to use
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is the capital of France?"
        }
    ],
    temperature=0.7
)

# Print the response
print(response.choices[0].message.content)
```

To run this example:
1. Install the SDK: `pip install atoma-sdk`
2. Set your API key as an environment variable: `export ATOMASDK_BEARER_AUTH=your_api_key`
3. Save the code in a file (e.g., `quickstart.py`)
4. Run the script: `python quickstart.py`

## Key Features

1. **OpenAI Compatibility**
   - Drop-in replacement for OpenAI API
   - Supports all standard OpenAI endpoints
   - Compatible with existing OpenAI client libraries

2. **Privacy and Security**
   - Support for confidential computing
   - Trusted Execution Environments (TEEs)
   - End-to-end encryption for sensitive data

3. **Distributed Infrastructure**
   - Global network of compute nodes
   - Intelligent load balancing
   - High availability and fault tolerance

## SDKs and Libraries

Atoma provides official SDKs for easy integration:

### TypeScript/JavaScript
```typescript
import { AtomaSDK } from "atoma-sdk";

const atomaSDK = new AtomaSDK({
  bearerAuth: process.env["ATOMASDK_BEARER_AUTH"] ?? "",
});

async function run() {
  const result = await atomaSDK.chat.create({
    messages: [
      { role: "user", content: "Hello!" }
    ],
    model: "meta-llama/llama-3.3-70b-instruct"
  });
  console.log(result);
}
```

### Python
```python
from atoma_sdk import AtomaSDK
import os

with AtomaSDK(
    bearer_auth=os.getenv("ATOMASDK_BEARER_AUTH", ""),
) as atoma_sdk:
    result = atoma_sdk.chat.create(
        messages=[
            {"role": "user", "content": "Hello!"}
        ],
        model="meta-llama/llama-3.3-70b-instruct"
    )
    print(result)
```

## Next Steps

- Explore the detailed API reference for each endpoint
- Learn about [confidential computing](/cloud-api-reference/confidential-computing) features
- Check out our [example applications](/cloud-api-reference/examples)