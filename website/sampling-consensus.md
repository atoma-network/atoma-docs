##  Issue with Trust 

In a permissionless network, no assumptions can be made about the accuracy of the AI generated output.  If we ask the network to run inference using Llama2-70B, how can we be assured that itâ€™s not actually using LLama2-13B? 

Verifiable inference exists so that models not only respond to prompts but prove that it ran correctly on the desired model. 

Maximum individual transaction size for blockcains is a few kilobytes whereas these AI models require hundreds of gigabytes. It is not feasible to run AI inference on chain therefore we have to look into alternative methods of verification. 

# Overview on Current Consensus Algorithms  

##  Byzantine Fault Tolerance

Byzantine Fault Tolerant consensus (BFT) is a method used in distributed systems to agree on a single data value or decision even if a proportion of the nodes in the system are unreliable or malicious. BFT consensus ensures that even if some participants are unreliable, the system as a whole can still make a reliable and unanimous decision.

The issue with BFT consensus is that the large majority of nodes (usually around 2/3rds) must agree on a transaction or block of transactions.

In the context of AI inference, BFT methods require heavy compute distributed across many nodes with each node performing the same computations independently. This incurs problems with computational power, latency, and scalability.

##  Zero Knowledge Machine Learning 

Zero knowledge machine learning (ZKML) is the application of zero knowledge proofs (ZKPs) to AI and machine learning models to ensure that the results of the machine learning models are correct. The details of the machine learning model itself (like the model weights) can remain confidential. 
 
ZKML is much slower because of the encryption methods used and more computationally intensive. Essentially, compiling a deep neural network into zk circuits for proofs is incredibly expensive and difficult. 

Given the demand for inference, ZKML is not scalable enough to offer fast and low cost results. 

##  Optimistic Machine Learning (OpML) 

OpML assumes everything is correct unless proven otherwise. Systems operate assuming computations are valid which initially is more efficient because it avoids the cost and delays incurred by immediate verification

In order to verify however, OpML has challenge periods which are specific time windows where participants in the system can challenge the validity of a computation and if a challenge is validated, a correction is made to ensure accuracy. 

These challenge periods lead to delays in finalizing computations because they allow time for challenges to be raised and resolved. The delay can vary in length preventing immediate finality. 

Immediate finality ensures that once a computation is confirmed, it is considered complete and valid across the entire system. Challenge periods delay this finality leading to inconsistent states if challenges alter the initial assumptions. The outcome of a computation might change after assuming correctness which disrupts any subsequent operations. 

OpML isn't an optimcal protocol for rewarding nodes as verifiers need to confirm the outputes by computing them. They receive rewards in the case that a disputed is resolved in their favour.  

##  Atoma's Sampling Consensus

Atoma has developed a novel sampling consensus protocol that eliminates the aforementioned issues.   

In order to process requests, the Atoma Network protocol selects a given number of nodes uniformly at random. The selected nodes must execute the given requests, otherwise part of their collateral value is at risk of being slashed. 

The selected nodes are guaranteed to have identical hardware and software specifications and the node sampling mechanism is part of our sampling consensus protocol. 

The Atoma Network sampling consensus protocol satisfies the following: 

1.  Load balancing of the request volume across the Atoma Network. 
- Requests are only executed across nodes with specific hardware capacities, this node selection homogenizes the balancing of the request load on the Atoma Network.

2.  Nodes reach deterministic agreement on the state of the output generated by executing the given request through the selection mechanism. 
- All selected nodes will generate the same outputs given the same inputs and the same workload (specified by the incoming request). This is especially important for workloads that rely on floating point arithmetic, the latter being highly non-deterministic.
- In the case for AI inference, there is a crucial dependence on floating point arithmetic (specially for LLM inference). By selecting nodes with the same hardware specifications (such as the same GPU card), determinism of floating point arithmetic is guaranteed (with potential mild changes to the kernels which the code runs into). 
- This is especially relevant to enforce verifiable compute. 

3.  Whenever two or more nodes are selected to execute a request, the selected nodes disagree if at least one of the nodes has been dishonest. 
- In particular, an honest node will never agree on the state of a request output with the dishonest one.

## Probabilistic Considerations for Atoma's Sampling Consensus

As previously mentioned, the Atoma protocol can enforce determinism for execution workloads (a.k.a requests) by replicating compute. It does this by randomly selecting a given number of nodes to run the same compute.

Depending on the number of sampled nodes, different levels of output integrity are achieved. For example, assuming a dishonest participant of the Atoma Network controls a percentage r of the network, the probability, P, that a quorum of N > 0 selected nodes (including at least one of the dishonest participant nodes) is:

P = r^N.

In particular, if N = 5 nodes are selected and the dishonest participant controls one third of the network, that is r = 1/3, the probability above becomes

P = (1/3)^5 = 0.00411..

with N = 10, the probability becomes

P = (1/3)^10 = 1.6935e-05,

We can see that even a small set of nodes, selected uniformly at random, can lead to very high trust guarantees that a given generated output has not been tampered with, in any possible form.


## Cross validation mechanism

## Node obfuscation


