# Atoma's Sampling Consensus

In order to process requests, the Atoma Network protocol selects a given number of nodes uniformly at random. The selected nodes must execute the given requests, otherwise part of their collateral value is at risk of being slashed. 

The selected nodes are guaranteed to have similar hardware and software specifications and the node sampling mechanism is part of our Sampling Consensus protocol. The latter allows the Atoma Network to satisfy the following features:

1. Load balancing of the request volume across the Atoma Network. As requests are only executed across nodes with specific hardware capacities, this node selection homogenizes the balancing of the request load on the Atoma Network.

2. The selection mechanism allows nodes to reach deterministic agreement on the state of the output generated by executing the given request. This means that all selected nodes will generate the same outputs given the same inputs and the same workload (specified by the incoming request). This is especially important for workloads that rely on floating point arithmetic, the latter being highly non-deterministic. An important example regards AI inference, which depends crucially on floating point arithmetic (specially for LLM inference). By selecting nodes with the same hardware specifications (such as the same GPU card), determinism of floating point arithmetic is guaranteed (with potential mild changes to the kernels which the code runs into).

This is especially relevant to enforce verifiable compute. By enforcing determinism across processed requests, we can rely on the following observation:

Observation: Through the Atoma sampling consensus protocol, whenever two or more nodes are selected to execute a request, the selected nodes disagree if at least one of the nodes has been dishonest. In particular, an honest node will never agree on the state of a request output with the dishonest one.

## Probabilistic Considerations for Atoma's Sampling Consensus

As previously mentioned, the Atoma protocol can enforce determinism for execution workloads (a.k.a requests) by replicating compute. It does this by randomly selecting a given number of nodes to run the same compute.

Depending on the number of sampled nodes, different levels of output integrity are achieved. For example, assuming a dishonest participant of the Atoma Network controls a percentage r of the network, the probability, P, that a quorum of N > 0 selected nodes (including at least one of the dishonest participant nodes) is:

P = r^N.

In particular, if N = 5 nodes are selected and the dishonest participant controls one third of the network, that is r = 1/3, the probability above becomes

P = (1/3)^5 = 0.00411..

with N = 10, the probability becomes

P = (1/3)^10 = 1.6935e-05,

We can see that even a small set of nodes, selected uniformly at random, can lead to very high trust guarantees that a given generated output has not been tampered with, in any possible form.


## Cross validation mechanism

## Node obfuscation


