##  Issue with Trust 

In a decentralized network, you can’t just run an LLM and trust the output. If we ask the network to run inference using Llama2-70B, how can we be assured that it’s not actually using LLama2-13B? 

Verifiable inference exists so that models not only respond to prompts but prove that it ran correctly on the desired model. 

It’s technically possible to run LLM inference as a smart contract on-chain, but it would incur huge costs. Therefore we have to look into alternative methods of verification. 

# Alternative methods of consensus 

##  Byzantine Fault Tolerance

Byzantine Fault Tolerance (BFT) consensus is a method used in distributed systems to agree on a single data value or decision even if a proportion of the nodes in the system are unreliable or malicious. BFT consensus ensures that even if some participants are unreliable, the system as a whole can still make a reliable and unanimous decision.

The issue with BFT consensus is that the large majority of nodes (usually around 2/3rds) must agree on a transaction or block of transactions.

In the context of AI inference, BFT methods require heavy compute distributed across many nodes with each node performing the same computations independently. This incurs problems with computational power, latency, and scalability.

##  Zero Knowledge Machine Learning 

Zero knowledge machine learning (ZKML) is a technique to ensure data remains private through zero knowledge proofs (ZKPs). When training the LLM, computations are completed without revealing any of the information provided. 

ZKML is much slower because of the encryption methods used and more computationally intensive. Essentially, compiling a deep neural network into zk circuits for proofs is incredibly expensive and difficult. 

Given the demand for inference, ZKML is not scalable enough to offer fast and low cost results. 

##  Optimistic Machine Learning (OPML) 

OPML approaches computations assuming everything is correct unless proven otherwise. Systems operate assuming computations are valid which initially is more efficient because it avoids the cost and delays incurred by immediate verification

In order to verify however, OPML has challenge periods which are specific time windows where participants in the system can challenge the validity of a computation and if a challenge is validated, a correction is made to ensure accuracy. 

These challenge periods lead to delays in finalizing computations because they allow time for challenges to be raised and resolved. The delay can vary in length preventing immediate finality. 

Immediate finality ensures that once a computation is confirmed, it is considered complete and valid across the entire system. Challenge periods delay this finality leading to inconsistent states if challenges alter the initial assumptions. The outcome of a computation might change after assuming correctness which disrupts any subsequent operations. 


##  Atoma's Sampling Consensus

Atoma has developed a novel sampling consensus protocol that eliminates the aforementioned issues.   

In order to process requests, the Atoma Network protocol selects a given number of nodes uniformly at random. The selected nodes must execute the given requests, otherwise part of their collateral value is at risk of being slashed. 

The selected nodes are guaranteed to have identical hardware and software specifications and the node sampling mechanism is part of our sampling consensus protocol. 

The Atoma Network sampling consensus protocol satisfies the following: 

1.  Load balancing of the request volume across the Atoma Network. 
- Requests are only executed across nodes with specific hardware capacities, this node selection homogenizes the balancing of the request load on the Atoma Network.

2.  Nodes reach deterministic agreement on the state of the output generated by executing the given request through the selection mechanism. 
- All selected nodes will generate the same outputs given the same inputs and the same workload (specified by the incoming request). This is especially important for workloads that rely on floating point arithmetic, the latter being highly non-deterministic.
- An important example regards AI inference, which depends crucially on floating point arithmetic (specially for LLM inference). By selecting nodes with the same hardware specifications (such as the same GPU card), determinism of floating point arithmetic is guaranteed (with potential mild changes to the kernels which the code runs into). 
- This is especially relevant to enforce verifiable compute. 

3.  Whenever two or more nodes are selected to execute a request, the selected nodes disagree if at least one of the nodes has been dishonest. 
- In particular, an honest node will never agree on the state of a request output with the dishonest one.

## Probabilistic Considerations for Atoma's Sampling Consensus

As previously mentioned, the Atoma protocol can enforce determinism for execution workloads (a.k.a requests) by replicating compute. It does this by randomly selecting a given number of nodes to run the same compute.

Depending on the number of sampled nodes, different levels of output integrity are achieved. For example, assuming a dishonest participant of the Atoma Network controls a percentage r of the network, the probability, P, that a quorum of N > 0 selected nodes (including at least one of the dishonest participant nodes) is:

P = r^N.

In particular, if N = 5 nodes are selected and the dishonest participant controls one third of the network, that is r = 1/3, the probability above becomes

P = (1/3)^5 = 0.00411..

with N = 10, the probability becomes

P = (1/3)^10 = 1.6935e-05,

We can see that even a small set of nodes, selected uniformly at random, can lead to very high trust guarantees that a given generated output has not been tampered with, in any possible form.


## Cross validation mechanism

## Node obfuscation


