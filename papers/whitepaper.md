# Atoma Whitepaper

## Authored by Jorge Antonio and Hisham Khan

### Abstract

Atoma is a permissionless decentralized protocol specialized for heavy verifiable compute. Atoma derives its features from compute providers, referred to as execution nodes (or simply nodes). Nodes on the Atoma Network are responsible for processing different types of compute requests. Atoma Network is specialized for AI inference compute, which includes deploying and serving AI compute (mostly in the form of AI inference) in a fully decentralized manner.

AI inference requests often require running Large Language models (LLMs) on different types of inputs (text, images, video, audio, etc) and produce some form of output (either in text, video, or other formats for multi-modality capabilities). More traditional AI models will also be available on the Atoma Network, specially those that are well suited for time series forecasting, recommendation systems, etc.

Due to the recent advances in the field of generative AI, the demand for AI compute is expected to grow exponentially in the coming years. Such tendency will inevitably lead to a rise in the demand for compute and energy resources.

More precisely, the rapid growth in AI model performance has been accompanied by an ever increasing number of model weight parameters, requiring higher amounts of memory to host such models. With current state of art LLMs requiring a few hundreds GBs, it is likely that this trend will only accelerate, and we will see much larger models being available in the future. For this reason, to run the best AI models at scale, it is crucial to have specialized powerful hardware, such as (multiple) high caliber GPUs (such as the Hopper and Blackwell Nvidia GPU series). 

In order to account for all the above phenomena, we have throughly designed Atoma to provide the right financial incentives to compute providers to participate in the network and be able to monetize their hardware, which in turn can be used as means to acquire better  hardware over the long run, which will allow nodes to accrue higher value in the future. Such financial incentives combined with a series of optimizations around AI inference will lead to close to optimal node's GPU utilization, allowing the Atoma Network to scale horizontally with the number of nodes, but also to scale vertically with better available hardware per node.

Through the Atoma Network, new applications will be built. Applications that can leverage the immense potential of generative AI models in order to provide a new intelligence layer for Web3 protocols. The Atoma Network will be a natural framework in order to build innovative applications such as protocol based AI generated content, AI market predictions, Web3 AI enhanced wallets specialized for user intent interactions, knowledge bases as public goods, social discussion forums, DAO and Network States governance, etc. 

In a world where AI will play the role of our caretaker, our tutor, our personal assistant and coworker, etc, it will be crucial that such applications will provide the high trust guarantees around AI inference.

That said, in order for the Atoma Network to become a full intelligence layer powering Web3 protocols, it is required to establish high integrity guarantees around AI inference compute. In practice, this means that AI generated outputs can be proved to have not been tampered in any possible form or, in other words, that no malicious actor could have interfered with the correct execution of an AI model. For this reason, we have established a novel sampling consensus protocol to ensure that Atoma Network nodes are able to provide high levels of verifiability for each AI compute request they process. Such sampling consensus protocol, referred to as `Sampling Consensus`, is based on the assumption that nodes are rational actors in a competing economic environment. Such protocol is optimized to achieve very high security guarantees, while having lower cost compared to other methods (such as zkML, opML, etc).

For the above reasons, the Atoma Network is well positioned to lead the space of decentralized AI compute. We deeply wish that through its development, the Atoma Network will open the doors for a more bright, transparent, innovative and democratic future where technology is fully aligned with users and communities needs.

### Introduction

The past few years have seen the emergence of a tremendous amount of technological innovations. These include new generative AI capabilities, powered by large language models (LLMs), more powerful hardware models which allow for massive scaling computing power, etc.

To put things into perspective, McKinsey estimates that generative AI could add the equivalent of $2.6 trillion to $4.4 trillion annually across the 63 use cases they analyzed. By comparison, the United Kingdom’s entire GDP in 2021 was $3.1 trillion. This would increase the impact of all artificial intelligence by 15% to 40%.
 
This growth will be accompanied by a rise in the demand for compute and energy resources. Over the recent months, we have seen a rise in the awareness by a few the experts around the need for governments and big tech companies to cooperate in order to build trillion dollar compute clusters, with the main goal of serving very large scale AI pipelines. We suggest the curious reader to read Leopold Aschenbrenner's most recent work on [Situational Awareness](https://situational-awareness.ai/wp-content/uploads/2024/06/situationalawareness.pdf).

If such trillion dollar compute clusters will materialize in the future, is yet to be seen. What is clear, however, is that the coming years will be accompanied by a tremendous rise in the demand for compute and energy resources, altogether.
Moreover, it is important to notice that AI pipelines require specialized hardware, such as GPUs, TPUs, etc. Even though GPUs are widely accessible to the public, most for gaming purposes, the
type of GPUs required for AI usually are of high caliber, and therefore less widely available to retail consumers and being also orders of magnitude more expensive than consumer grade ones. This observation combined with the fact that there are only a few GPU manufacturers (such as NVIDIA and AMD) and that chip manufacturing is currently being affected by international geopolitical tensions will lead to less effective distribution of compute. If follows from these that high caliber GPUs will continue to be of difficult access to retail and even small to medium size data center businesses, while allowing GPU manufacturers to keep very high profit margins (claimed to be around 90% by NVIDIA [REF]).

If these dynamics will persist in the future, it is likely that only governments and big tech companies will retain most of the world's available compute resources. In such a scenario, these institutions will have a complete monopoly over most of AI development, from model training to inference, etc. In the meantime, such institutions will want to preserve their margin of profit and or their intellectual property for competing reasons, which will inevitably make AI a closed source technology (that is, model weights are not available to the general public).

Combined with the fact that big tech companies have access to virtually most of the real world data being generated, we will see an extreme centralization of the most revolutionary technology of our time, generative AI. Such scenario will only make it more likely to AIs to become less aligned with users needs and values, increasing the risk of AI misalignment for the general public. This is a very dangerous future scenario, and the only way to avoid it is to build more fair and transparent technologies, technologies that can help to mitigate the centralization risks mentioned above.

This is where decentralized protocols can play an important role, in mitigating some of the risk factors above. For example, we have witnessed financial incentives alignment through blockchains such as Bitcoin, Ethereum, etc, with massive network growth effects. 

Moreover, we have also witnessed tremendous innovation in blockchain technologies, as well. With new highly scalable blockchains, allowing for lower transaction fees, through a combination of novel consensus algorithms, cryptography and modularity techniques, etc. 

These technologies provide the right framework to align incentives between compute, financial resources and communities. Inspired by the rise of decentralized applications, we are committed to build the Atoma Network, a permissionless decentralized protocol specialized for heavy verifiable AI compute. The goal of the Atoma Network is to allow compute providers to be able to monetize their compute resources in a permissionless and transparent environment, while being exposed to fair market economic incentives. In order to achieve this, we are building Atoma's node infrastructure with a focus on the ease of software deployment, which allows to execute AI compute requests, through the Atoma Network. Such infrastructure is highly optimized for best GPU utilization resources, incorporating the most recent advances in LLM memory management techniques (such as Paged Attention, quantization techniques, etc).

On the other hand, the Atoma Network will provide a platform for bringing demand for the usage of such compute resources themselves, working effectively as an AI market place. We plan to drive demand for Atoma's compute resources by building decentralized infrastructure which serves verifiable AI compute. That is, Atoma nodes will be able to process AI requests while providing strong verifiable guarantee attestations that the AI generated output has not been tampered in any form whatsoever. Such attestations can be used by users to verify the state of the output, and by Web3 protocols to ensure that generated outputs are indeed legitimate. 

Through these features, Web3 protocols will be able to have access to AI functionality, through the Atoma Network, allowing to integrate a true intelligence layer into Web3 dApps. In order to provide verifiability, we have developed a novel sampling consensus protocol, specialized in output state verifiability. Such consensus protocol is elastic, in the sense, that the end user is able to 
decide how much of output integrity guarantees their use case requires, paying therefore a fee that is aligned with such level of verifiability. Through this, Atoma Network will be able to empower Web3 applications, as well as naturally centralized Web2 applications (those requiring no verifiability at all). 

That said, there will be demand for Atoma Network functionality beyond direct smart contracts usage. Applications such as UI chat applications, AI Web3 wallet integrations, online data scrapping,
next generation browsers and browser extensions, community based knowledge bases, social media platforms, etc will benefit from verifiable generative AI.

Contrary to Web2 companies, nodes operating on a decentralized network are not necessarily under the of national regulations. Therefore,
applications which naturally do not demand the same level of output integrity, can’t necessarily trust being operated by a single centralized party to perform that inference due to the potential for misbehaving nodes. For this reason, such applications still need to be subject to a decentralized consensus style system.

As mentioned above, applications like chat-bots or smart agents scraping the web will not require the same level of trust assumptions as general smart contracts, but will still require certain integrity guarantees. Whereas, a DeFi protocol may require almost 100% certainty that a given AI market liquidity prediction was computed correctly. Moreover, a node serving the given prediction might have a position contrary to that prediction, which creates a high financial incentive for the given node to try to tamper the generated output, in order to serve its own personal interests. Therefore, in such a case the incentive for the node to cheat the protocol might be as high as the total liquidity of such DeFi protocol. Moreover, even a small deviation from the original prediction can have a huge impact on the protocol's future funds management. For these reasons, it is of uttermost importance that Atoma's verifiability can be as high as possible.

On the contrary, a user interacting with an online community based chat application is mostly interested in knowing that the fee it paid to have access to AI functionalities throughout a 70 billion parameter model is not being served instead by a 7 billion parameter model, making the user effectively paying more for a worse service. In such scenario, verifiability guarantees need to be as high as in the previous example, and the system can work securely with a Nash equilibrium over the long run, with little replication overhead costs, which in practical terms means most requests being operated at native cost, while still maintaining a high level of security guarantees.

Our Sampling Consensus algorithm is well adapted for each of the above use cases, being highly flexible and optimized for a wide range of different scenarios. That said, we are also actively exploring different verifiability techniques, such as Trusted Execution Environments (TEEs), that will allow for even higher levels of verifiability, while also reducing the overhead cost of verifiable execution to native cost.

That said, Web2 style applications will most likely drive the demand for the Atoma Network in the short to medium term. This derives from the fact that Web3 applications are still in their infancy in terms of AI adoption. In numbers, the total market for AI (comprising not only generative AI) is estimated be worth in the tens of trillions of dollars, worldwide. At today's time of writing, the Web3 market is estimated to be worth $2.7 trillion today. We believe that in order to drive mass adoption of Web3, it is crucial to reach parity with the type of UX and UI that Web2 applications offer, today. Most of such ease of UI and UX comes due AI integration into Web2 applications backends. Therefore, we are lead to believe that Web3 protocols will need to integrate these AI capabilities in order to onboard a larger number of users.

In order to empower the next generation applications lying at the intersection of Web3 and AI, both data and model privacy is of utmost importance. Through the use of TEEs, we will not only be able to provide verifiable AI compute, but also provide full data privacy to the end user. This means that Atoma nodes will not be able to access any sensitive data, such as personal user information, while also not being able to access the output generated by the AI model. Only the end user will be able to decrypt the generated output. Privacy for AI compute will invariably lead to a users owned full control of their own data. The latter will unlock the full potential of AI, allowing for the creation of both new data and knowledge economic revenue models, as well as true forms of automation, in which AI models can operate on the behalf of users without ever revealing any sensitive data about the user. 

The Atoma Network can be used by Web2 companies and builders, specially software companies that are built from the ground up to be AI-centric, fast growing AI providers (such as Anthropic, etc) and software companies committed to integrating AI into their products (think of Canvas, etc). The reason for these companies to want to operate on the Atoma Network comes from the fact that a network like Atoma is positioned to drive down the costs of GPU utilization. 

Due to the current GPU chips shortage, together with a waste of GPU utilization (meaning less GPU cycles being used for AI inference), hardware providers face sunk costs by not utilize their infrastructure efficiently. Therefore, such hardware providers are able to offer those fractional GPU cycles at a much lower cost compared to other larger cloud providers. Notice, for example, that renting an H100 from AWS today, customers need to commit to a 1-year lease, due to market supply constraints. It is also unlikely that a customer needs to use a GPU at full capacity during the 1-year period. There is furthermore, a high number of idle GPUs in the market, that were used before as mining hardware (e.g., when ETH transitioned from PoW to PoS). Such hardware available can be repurposed for AI inference.

The Atoma Network will be empowered by its TOMA token. Through a robust tokenomics, both compute providers, protocol participants and developers will be rewarded by their contribution to maintaining the network longevity. The TOMA token will be required to pay for AI inference fees, for nodes to deposit collateral in order to be able to participate into the Atoma Network, as well as for developers to accrue value for their contributions to the Atoma Network. 

Moreover, the development of new AI models will be crucial for the future and longevity of the Atoma Network. We believe that open source AI development will be crucial to democratize and broaden the reach of AI technologies across communities and societies. We are committed to incentivize the open source AI community to develop and deploy new AI models, while being able to monetize their work, allowing for experts in the community to earn a continuous stream of revenue through their model utilization. In the past, open source software has been shown to drive technology innovation at a much faster pace than closed source software alternatives. We believe that the same effect will apply to AI model development.

This is especially important once we factor in possible risks of AI censorship. For example, Italy [banned](https://www.bbc.com/news/technology-65139406) the use of OpenAI's ChatGPT shortly after its release, due to user privacy concerns (even though it later lifted the ban). On the other hand, companies like OpenAI are self-censoring due to societal and political reasons. For example, OpenAI will not predict the next presidential election, etc. These use cases will likely create large predictions/content markets that big technology companies will not want to be involved in. The Atoma Network, on the other hand, can leverage open-source software AI models, at a lower-cost GPU cluster to perform inference for these types of use cases.

The token distribution will follow an initial period of inflation in order to subsidize the network's growth. This growth will be used to subsidize future node operators to acquire powerful AI hardware, to incentivize model creators to develop new AI models, specialized for applications of interest to the Atoma Network (such as financial market predictions, etc), as well as to incentivize developers to build applications that drive demand for the available compute resources available on the Atoma Network. After the network's inflationary period (which will last a few years), the token emission distribution will follow a slightly deflationary dynamics, through a combination of tail emissions and burning mechanisms. Together, both strategies will lead to a gradual decrease in the total supply of the TOMA token, contributing for its overall value increase (provided the demand for Atoma Network increases), while also allowing for the allocation of the very small portion of newly minted tokens to the protocol's treasury that can then be used for different purposes, which can include bringing new generation hardware to the network, social incentives through UBI, etc.

A decentralized platform such as the Atoma Network will put forth combined efforts that will ultimately lead to a new era of use cases within decentralized applications, driving financial incentives for computing power to be partially decouple from centralized cloud providers. Smart contracts will not only deploy new financial products, but also be able to deploy and orchestrate compute resources, powering new decentralized protocols. Our aim is to build parts of this future, we are incredibly excited about the future of the Atoma Network, and decentralized permissionless protocols for general compute tasks.

The focus of the current text is to explore at a deeper level the Atoma's unique design and architecture. More concretely, we will provide detailed descriptions on the following topics:

* Atoma Network as a decentralized permissionless protocol for AI inference.
* Verifiable AI compute methods, including our novel sampling consensus protocol and TEEs.
* Data and privacy protection for AI inference.
* Governance and reputation mechanisms for Atoma Network participants.
* Node infrastructure and optimizations.
* Applications and use cases for Atoma Network.
* Tokenomics and economic incentives for Atoma Network participants.

### Atoma Network as a decentralized permissionless protocol for AI inference

