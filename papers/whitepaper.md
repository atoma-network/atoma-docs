# Atoma Whitepaper

## Authored by Jorge Antonio and Hisham Khan

### Abstract

Atoma is a permissionless decentralized protocol specialized for heavy verifiable compute. Atoma derives its features from compute providers, referred to as execution nodes (or simply nodes), which are responsible for processing different types of compute requests. Atoma Network is specialized for AI inference compute, which includes deploying and serving AI compute (mostly in the form of AI inference) in a fully decentralized manner.

AI inference requests are often processed through nodes that run Large Language models (LLMs) on different types of inputs (text, images, video, audio) to produce some form of output (which can be either text, or other formats for multi-modality capabilities). The Atoma Network will also support more conventional and traditional models for time series predictions, recommendation systems, etc.

Due to the recent advances in the field of generative AI, the demand for AI compute is expected to grow exponentially in the coming years. Such tendency will inevitably lead to a rise in the demand for compute and energy resources.

More precisely, the rapid growth in AI model performance has been accompanied by an ever increasing number of model weight parameters, requiring higher amounts of memory to host such models. With current state of art LLMs requiring a few hundreds GBs, it is likely that this trend will only accelerate, and we will see much larger models being available in the future. For this reason, to run the best AI models at scale, it is crucial to have specialized powerful hardware, such as (multiple) high caliber GPUs (such as the Hopper and Blackwell Nvidia GPU series). 

In order to account for all the above phenomena, we have throughly designed Atoma to provide the right financial incentives to compute providers to participate in the network and be able to monetize their hardware, which in turn can be used as means to acquire better  hardware over the long run, which will allow nodes to accrue higher value in the future. Such financial incentives combined with a series of optimizations around AI inference will lead to close to optimal node's GPU utilization, allowing the Atoma Network to scale horizontally with the number of nodes, but also to scale vertically with better available hardware per node.

Through the Atoma Network, new applications will be built. Applications that can leverage the immense potential of generative AI models in order to provide a new intelligence layer for Web3 protocols. The Atoma Network will be a natural framework in order to build innovative applications such as protocol based AI generated content, AI market predictions, Web3 AI enhanced wallets specialized for user intent interactions, knowledge bases as public goods, social discussion forums, DAO and Network States governance, etc. 

In a world where AI will play the role of our caretaker, our tutor, our personal assistant and coworker, etc, it will be crucial that such applications will provide the high trust guarantees around AI inference.

That said, in order for the Atoma Network to become a full intelligence layer powering Web3 protocols, it is required to establish high integrity guarantees around AI inference compute. In practice, this means that AI generated outputs can be proved to have not been tampered in any possible form or, in other words, that no malicious actor could have interfered with the correct execution of an AI model. For this reason, we have established a novel sampling consensus protocol to ensure that Atoma Network nodes are able to provide high levels of verifiability for each AI compute request they process. Such sampling consensus protocol, referred to as `Sampling Consensus`, is based on the assumption that nodes are rational actors in a competing economic environment. Such protocol is optimized to achieve very high security guarantees, while having lower cost compared to other methods (such as zkML, opML, etc).

For the above reasons, the Atoma Network is well positioned to lead the space of decentralized AI compute. We deeply wish that through its development, the Atoma Network will open the doors for a more bright, transparent, innovative and democratic future where technology is fully aligned with users and communities needs.

### Introduction

The past few years have seen the emergence of a tremendous amount of technological innovations. These include new generative AI capabilities powered by large language models (LLMs), more powerful hardware models which allow for massive scaling computing power, and a rise of highly scalable blockchains with lower transaction fees that utilize a combination of novel consensus, cryptography, and modularity techniques.

To put things into perspective, McKinsey estimates that generative AI could add the equivalent of $2.6 trillion to $4.4 trillion annually across the 63 use cases they analyzed. By comparison, the United Kingdom’s entire GDP in 2021 was $3.1 trillion. This would increase the impact of all artificial intelligence by 15% to 40%.
 
This growth will be accompanied by a rise in the demand for compute and energy resources. Through the first half of 2024, we have seen experts in the field highlight the need for governments and big tech companies to cooperate in order to build the trillion dollar compute cluster. We refer to the detailed analysis of this question by Leopold Aschenbrenner in his most recent work on Situational Awareness.

Whether this trillion dollar compute cluster will materialize in the future is yet to be seen. What is clear, however, is the rise of demand for compute and energy resources, over the coming years. AI pipelines require specialized hardware, such as GPUs, TPUs, etc. Even though GPUs are widely accessible to the public, most for gaming purposes, the
type of GPUs required for AI usually are of high caliber, and therefore less widely available to retail consumers and orders of magnitude more expensive. There are only a few GPU manufacturers (such as NVIDIA and AMD), and there are geopolitical tensions that directly affect the supply chains of producing these devices. This results in difficulty making high caliber GPUs for AI accessible to retail or small to medium size data center businesses. If this trend continues, we will step into a future in which governments and big tech companies retain most of the world's available compute resources whilst also owning the next generation AI models.

If these dynamics will persist in the future, it is likely that only governments and big tech companies will retain most of the world's available compute resources. In such a scenario, these institutions will have a complete monopoly over most of AI development, from model training to inference, etc. In the meantime, such institutions will want to preserve their margin of profit and or their intellectual property for competing reasons, which will inevitably push AI towards a closed source technology (that is, model weights are not available to the general public).

Combined with the fact that big tech companies have access to virtually most of the real world data being generated, we will see an extreme centralization of the most revolutionary technology of our time, generative AI. Such scenario will only make it more likely to AIs to become less aligned with users needs and values, increasing the risk of AI misalignment for the general public. This is a very dangerous future scenario, and the only way to avoid it is to build more fair and transparent technologies, technologies that can help to mitigate the centralization risks mentioned above.

This is where decentralized protocols can play an important role, in mitigating some of the risk factors above. For example, we have witnessed financial incentives alignment through blockchains such as Bitcoin, Ethereum, etc, with massive network growth effects. 

These technologies provide the right framework to align incentives between compute, financial resources and communities. Inspired by the rise of decentralized applications, we are committed to build the Atoma Network, a permissionless decentralized protocol specialized for heavy verifiable AI compute. The goal of the Atoma Network is to allow compute providers to be able to monetize their compute resources in a permissionless and transparent environment, while being exposed to fair market economic incentives. In order to achieve this, we are building Atoma's node infrastructure with a focus on the ease of software deployment. Atoma node infrastructure will allow nodes to easily execute AI compute requests, through the Atoma Network. Such infrastructure is highly optimized for best GPU utilization resources, incorporating the most recent advances in LLM memory management techniques (such as Paged Attention, quantization techniques, etc).

On the other hand, the Atoma Network will provide a platform that creates demand for using compute, working effectively as an AI market place. We plan to achieve this by building decentralized infrastructure that provides verifiable AI compute, that is, any AI request can be followed with an attestation asserting that the AI generated output has not been tampered in any form.

Through these features, Web3 protocols will be able to have access to AI functionality, through the Atoma Network, allowing to integrate a true intelligence layer into Web3 dApps. In order to provide verifiability, we have developed a novel sampling consensus protocol, specialized in output state verifiability. Such consensus protocol is elastic, in the sense, that the end user is able to 
decide how much of output integrity guarantees their use case requires, paying therefore a fee that is aligned with such level of verifiability. Through this, Atoma Network will be able to empower Web3 applications, as well as naturally centralized Web2 applications. 

Additionally there will be demand for Atoma Network functionality beyond smart contracts such as UI chat applications, AI Web3 wallet integrations, online data scraping, next generation browsers and browser extensions, community based knowledge bases, social media platforms, and more. These applications will not require the same level of trust assumptions as general smart contracts, but will still require certain integrity guarantees.

Contrary to Web2 companies, nodes operating on a decentralized network are not necessarily under the of national regulations. Therefore,
applications which naturally do not demand the same level of output integrity, can’t necessarily trust being operated by a single centralized party to perform that inference due to the potential for misbehaving nodes. For this reason, such applications still need to be subject to a decentralized consensus style system.

As mentioned above, applications like chat-bots or smart agents scraping the web will not require the same level of trust assumptions as general smart contracts, but will still require certain integrity guarantees. Whereas, a DeFi protocol may require almost 100% certainty that a given AI market liquidity prediction was computed correctly, even a small deviation from the original prediction can have a huge impact on the protocol's future funds management. Therefore, it is of uttermost importance that Atoma's verifiability can be as high as possible, in such cases.

On the contrary, a user interacting with an online community based chat application is mostly interested in knowing that it paid the right fee to have access to the right model for output generation, and not a less capable model. In such scenario, the risk for the user and the potential reward for malicious behavior for the serving node are much lower than in the previous example. In such scenario, the system can work securely with a Nash equilibrium over the long run, with little replication overhead costs, which in practical terms means most requests being operated at native cost, while still maintaining a high level of security guarantees.

Our Sampling Consensus algorithm is well adapted for each of the above use cases, being highly flexible and optimized for a wide range of different scenarios. That said, we are also actively exploring different verifiability techniques, such as Trusted Execution Environments (TEEs), that will allow for even higher levels of verifiability, while also reducing the cost of verifiable execution close to native cost.

That said, Web2 style applications will most likely drive the demand for the Atoma Network in the short to medium term. This derives from the fact that Web3 applications are still in their infancy in terms of AI adoption. In numbers, the total market for AI (comprising not only generative AI) is estimated be worth in the tens of trillions of dollars, worldwide. At today's time of writing, the Web3 market is estimated to be worth $2.7 trillion today. We believe that in order to drive mass adoption of Web3, it is crucial to reach parity with the type of UX and UI that Web2 applications offer, today. Most of such ease of UI and UX comes due AI integration into Web2 applications backends. Therefore, we are lead to believe that Web3 protocols will need to integrate these AI capabilities in order to onboard a larger number of users.

In order to empower the next generation of applications in the intersection of Web3 and AI, both data and model privacy is of the utmost importance. Through the use of TEEs, we will not only be able to provide verifiable AI compute, but also provide full data privacy to the end user.

This means that Atoma nodes will not be able to access any sensitive data, such as personal information, while also not being able to access the output generated by the AI model. Only the end user will be able to decrypt the generated output, and can have high assurance that the input to the model was not shared with any third party.

Through this, users can have full control of their data, allowing for AI models to operate on sensitive data. Privacy will unlock the full potential of AI, allowing for the creation of new data centric economic revenue models as well as applications that can operate on the user's behalf without revealing anything about the user.

The Atoma Network can be used by Web2 companies and builders, specially software companies that are built from the ground up to be AI-centric, fast growing AI providers (such as Anthropic, etc) and software companies committed to integrating AI into their products (think of Canvas, etc). The reason for these companies to want to operate on the Atoma Network comes from the fact that a network like Atoma is positioned to drive down the costs of GPU utilization. 

Due to the current GPU chips shortage, together with a waste of GPU utilization (meaning less GPU cycles being used for AI inference), hardware providers face sunk costs by not utilize their infrastructure efficiently. Therefore, such hardware providers are able to offer those fractional GPU cycles at a much lower cost compared to other larger cloud providers. Notice, for example, that renting an H100 from AWS today, customers need to commit to a 1-year lease, due to market supply constraints. It is also unlikely that a customer needs to use a GPU at full capacity during the 1-year period. There is furthermore, a high number of idle GPUs in the market, that were used before as mining hardware (e.g., when ETH transitioned from PoW to PoS). Such hardware available can be repurposed for AI inference.

The development of new AI models will be crucial for the future of the Atoma Network. We believe that open source AI development will be crucial to democratize and broaden the reach of AI technologies across societies. We are committed to incentivize the open source AI community to develop and deploy new AI models, while being able to monetize their work, allowing for experts in the community to earn a continuous stream of revenue through their model utilization.

This is especially important once we factor in possible risks of AI censorship. For example, Italy [banned](https://www.bbc.com/news/technology-65139406) the use of OpenAI's ChatGPT shortly after its release, due to user privacy concerns (even though it later lifted the ban). On the other hand, companies like OpenAI are self-censoring due to societal and political reasons. For example, OpenAI will not predict the next presidential election, etc. These use cases will likely create large predictions/content markets that big technology companies will not want to be involved in. The Atoma Network, on the other hand, can leverage open-source software AI models, at a lower-cost GPU cluster to perform inference for these types of use cases.

The Atoma Network will be powered by its TOMA token. Through robust tokenomics, compute providers, protocol participants, and developers will be rewarded by their contribution to maintaining the network longevity. The TOMA token will be required to pay for AI inference fees, for nodes to deposit collateral to be able to participate on the network, and also for developers to be paid for their contributions to the Atoma Network.

The token distribution will follow an initial period of inflation in order to subsidize the network's growth. This growth will be used to subsidize future node operators to acquire powerful AI hardware, to incentivize model creators to develop new AI models, specialized for applications of interest to the Atoma Network (such as financial market predictions, etc), as well as to incentivize developers to build applications that drive demand for the available compute resources available on the Atoma Network. After the network's inflationary period (which will last a few years), the token emission distribution will follow a slightly deflationary dynamics, through a combination of tail emissions and burning mechanisms. Together, both strategies will lead to a gradual decrease in the total supply of the TOMA token, contributing for its overall value increase (provided the demand for Atoma Network increases), while also allowing for the allocation of the very small portion of newly minted tokens to the protocol's treasury that can then be used for different purposes, which can include bringing new generation hardware to the network, social incentives through UBI, etc.

As a decentralized platform, Atoma Network will put forth combined efforts that will ultimately lead to a new era of use cases within decentralized applications through driving financial incentives for computing power to be partially decoupled from centralized cloud providers. Smart contracts will not only deploy new financial products, but also be able to deploy and orchestrate compute resources, powering new decentralized protocols.

The focus of the current text is to explore at a deeper level the Atoma's unique design and architecture. More concretely, we will provide detailed descriptions on the following topics:

* Atoma Network as a decentralized permissionless protocol for AI inference.
* Verifiable AI compute methods, including our novel sampling consensus protocol and TEEs.
* Data and privacy protection for AI inference.
* Governance and reputation mechanisms for Atoma Network participants.
* Node infrastructure and optimizations.
* Applications and use cases for Atoma Network.
* Tokenomics and economic incentives for Atoma Network participants.

### Atoma Network as a decentralized permissionless protocol for AI inference

