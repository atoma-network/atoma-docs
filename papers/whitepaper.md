# Atoma Whitepaper

## Authored by Jorge Antonio and Hisham Khan

### Abstract

Atoma is a permissionless decentralized protocol specialized for heavy verifiable compute. Atoma derives its features from compute providers, referred to as execution nodes (or simply nodes). Nodes on the Atoma Network are responsible for processing different types of compute requests. Atoma Network is specialized for AI inference compute, which includes deploying and serving AI compute (mostly in the form of AI inference) in a fully decentralized manner.

AI inference requests often require running Large Language models (LLMs) on different types of inputs (text, images, video, audio, etc) and produce some form of output (either in text, video, or other formats for multi-modality capabilities). More traditional AI models will also be available on the Atoma Network, specially those that are well suited for time series forecasting, recommendation systems, etc.

Due to the recent advances in the field of generative AI, the demand for AI compute is expected to grow exponentially in the coming years. Such tendency will inevitably lead to a rise in the demand for compute and energy resources.

More precisely, the rapid growth in AI model performance has been accompanied by an ever increasing number of model weight parameters, requiring higher amounts of memory to host such models. With current state of art LLMs requiring a few hundreds GBs, it is likely that this trend will only accelerate, and we will see much larger models being available in the future. For this reason, to run the best AI models at scale, it is crucial to have specialized powerful hardware, such as (multiple) high caliber GPUs (such as the Hopper and Blackwell Nvidia GPU series). 

In order to account for all the above phenomena, we have throughly designed Atoma to provide the right financial incentives to compute providers to participate in the network and be able to monetize their hardware, which in turn can be used as means to acquire better  hardware over the long run, which will allow nodes to accrue higher value in the future. Such financial incentives combined with a series of optimizations around AI inference will lead to close to optimal node's GPU utilization, allowing the Atoma Network to scale horizontally with the number of nodes, but also to scale vertically with better available hardware per node.

Through the Atoma Network, new applications will be built. Applications that can leverage the immense potential of generative AI models in order to provide a new intelligence layer for Web3 protocols. The Atoma Network will be a natural framework in order to build innovative applications such as protocol based AI generated content, AI market predictions, Web3 AI enhanced wallets specialized for user intent interactions, knowledge bases as public goods, etc. 

That said, in order for the Atoma Network to become a full intelligence layer powering Web3 protocols, it is required to establish high integrity guarantees around AI inference compute. In practice, this means that AI generated outputs can be proved to have not been tampered in any possible form or, in other words, that no malicious actor could have interfered with the correct execution of an AI model. For this reason, we have established a novel sampling consensus protocol to ensure that Atoma Network nodes are able to provide high levels of verifiability for each AI compute request they process. Such sampling consensus protocol, referred to as `Sampling Consensus`, is based on the assumption that nodes are rational actors in a competing economic environment. Such protocol is optimized to achieve very high security guarantees, while having lower cost compared to other methods (such as zkML, opML, etc).

For the above reasons, the Atoma Network is well positioned to lead the space of decentralized AI compute. We deeply wish that through its development, the Atoma Network will open the doors for a more bright, transparent, innovative and democratic future where technology is fully aligned with users and communities needs.

### Introduction

The past few years have seen the emergence of a tremendous amount of technological innovations. These include new generative AI capabilities, powered by large language models (LLMs), more powerful hardware models which allow for massive scaling computing power, and a rise of highly scalable blockchains, with lower transaction fees, through a combination of novel consensus, cryptography and modularity techniques, etc. 

To put things into perspective, it is estimate that the global market for AI is expected to grow by $100 billion by 2027, with a compound annual growth rate of 10.5%. This growth will be accompanied by a rise in the demand for compute and energy resources. Over the recent months, we have seen a rise in the awareness by a few the experts in the field around the need for governments and big tech companies to cooperate in order to build the trillion dollar compute cluster. We refer to the detailed analysis of this question by Leopold Aschenbrenner in his most recent work on [Situational Awareness](https://situational-awareness.ai/wp-content/uploads/2024/06/situationalawareness.pdf).

If such trillion dollar compute clusters will materialize in the future, is yet to be seen. What is clear, however, is the rise of demand for compute and energy resources, over the coming years.
Moreover, it is important to notice that AI pipelines require specialized hardware, such as GPUs, TPUs, etc. Even though GPUs are widely accessible to the public, most for gaming purposes, the
type of GPUs required for AI usually are of high caliber, and therefore less widely available to retail consumers and also orders of magnitude more expensive. This observation combined with the fact that there are only a few GPU manufacturers (such as NVIDIA and AMD), together with the fact that chip manufacturing is being affected by international geopolitical tensions will lead to less effective distribution of compute, which only contributes to make high caliber GPUs of difficult access to retail and even small to medium size data center businesses. If the demand for
compute is mostly met by big tech companies and governments, then altogether these dynamics will inevitably push higher the compute cost will be much higher. If this trend continues, we will step in a future in which only governments and big tech companies retain most of the world's available compute resources, and therefore, these institutions will have the monopoly over AI
deployment. Combined with the fact that such entities can have access to virtually most of the real world data, will lead to the an extreme centralization of the most revolutionary technology of our time, generative AI. Such scenario will only make it more likely to AIs to become less aligned with users needs and values, increasing the risk of AI misalignment for the general public. This is a very dangerous future scenario, and the only way to avoid it is to build more fair and transparent technologies, technologies that help to mitigate each of the centralization risks mentioned above.

This is where decentralized protocols can play an important role, in mitigating some of the risk factors above. For example, we have witnessed financial incentives alignment through blockchains such as Bitcoin, Ethereum, etc, with massive network growth effects. These technologies provide the right framework to align incentives between compute, financial resources and permissionless communities. Inspired by the rise of decentralized applications, we are committed to build the Atoma Network, a permissionless decentralized protocol specialized for heavy verifiable AI compute. The goal of the Atoma Network is to allow compute providers to be able to monetize their compute resources in the most permissionless and transparent way possible, while being exposed to fair market pricing. In order to achieve this, we are in the process of developing Atoma node's infrastructure, which allows any compute provider (also referred to as a node) to easily deploy software that allows to execute AI compute requests, through the Atoma Network. Such infrastructure is highly optimized for best GPU utilization resources, incorporating the most recent advances in LLM memory management techniques (such as Paged Attention and vLLM), etc.

On the other hand, the Atoma Network will provide a platform for bringing demand for the usage of such compute resources themselves, working effectively as an AI market place. We plan to drive demand for compute resources at Atoma Network, by building decentralized infrastructure that provides verifiable AI compute, that is, any AI request can be followed with an attestation asserting that the AI generated output has not been tampered in any form. Such attestation can be used by the user to verify the state of the output, and can be used by the protocol to ensure that the output is legitimate. This means that Web3 protocols will be able to have access to AI, through the Atoma Network, allowing to integrate a true intelligence layer into Web3 dApps. In order to provide verifiability, we have developed a novel sampling consensus protocol, specialized in output state verifiability. Such consensus protocol is elastic, in the sense, that the end user can
decide how much of verifiability their use case requires, paying therefore a fee that is proportional to such level of verifiability. Through this, Atoma Network will be able to empower Web3 applications, as well as naturally centralized Web2 applications. 

That said, there will be demand for Atoma Network functionality beyond smart contracts, directly. Applications such as UI chat applications, AI Web3 wallet integrations, online data scrapping,
next generation browsers and browser extensions, community based knowledge bases, social media platforms, etc. Such applications will not require the same level of trust assumptions as general smart contracts, but will still require certain integrity guarantees. As an example, a DeFi protocol may require almost 100% certainty that a given AI market liquidity prediction was computed correctly, even a small deviation from the original prediction can have a huge impact on the protocol's future funds management. However, a user interacting with an online community based chat application is mostly interested in knowing that it paid the right fee to have access to the right model for output generation, and not a less capable model. Therefore, our sampling consensus is adapted to each of such use cases, being therefore highly flexible and optimizing for a wide range of different use cases. We are currently exploring other verifiability techniques, such as Trusted Execution Environments (TEEs), that will allow for even higher levels of verifiability, while also reducing the cost of verifiable execution close to native cost.

The Atoma Network will be empowered by its TOMA token. Through a robust tokenomics, both compute providers, protocol participants and developers will be rewarded by their contribution to maintaining the network longevity. The TOMA token will be required to pay for AI inference fees, for nodes to deposit collateral to be able to participate on the network, and also for developers to being paid for their contributions to the Atoma Network. Moreover, the tokenomics follow an initial period of inflation in order to subsidize the network's growth. This growth includes both incentivizing node operators to acquire better compute hardware, to incentivize model creators to develop new AI models, specialized for applications of interest to the Atoma Network (such as financial market predictions, etc), as well as to incentivize developers to build applications that drive demand for the available compute resources on the Atoma Network. After the network's inflationary period (which will last a few years), the tokenomics will follow a more balanced dynamics, through tail emissions and burning mechanisms. Together, both mechanisms will lead to a gradual decrease in the total supply of the TOMA token, contributing for its increase in value provided the demand for Atoma Network increases, while also allowing for a some new allocation of the token to the protocol's treasury and to value providers (in the form of new node providers, model creators, developers, etc).

In order to empower the next generation applications in the intersection of Web3 and AI, both data and model privacy is of utmost importance. Through the use of TEEs, we will not only be able to provide verifiable AI compute, but also provide full data privacy to the end user. This means that Atoma nodes will not be able to access any sensitive data, such as personal information, while also not being able to access the output generated by the AI model. Only the end user will be able to decrypt the generated output, and can have high assurance that the input to the model was not shared with any third party. Through this, users can have full control of their data, allowing for AI models to operate on sensitive data. Privacy will unlock the full potential of AI, allowing for the creation of new data economic revenue models, as well applications that can operate on the user's behalf without revealing anything about the user. 

On another note, the development of new AI models will be crucial for the future of the Atoma Network. We believe that open source AI development will be crucial to democratize and broaden the reach of AI technologies across societies. We are committed to incentivize the open source AI community to develop and deploy new AI models, while being able to monetize their work, allowing for experts in the community to earn a continuous stream of revenue through their model utilization.

A decentralized platform such as Atoma Network will put forth combined efforts that will ultimately lead to a new era of use cases within decentralized applications, driving financial incentives for computing power to be partially decouple from centralized cloud providers. Smart contracts will not only deploy new financial products, but also be able to deploy and orchestrate compute resources, powering new decentralized protocols. 

The focus of the current text is to explore at a deeper level the following topics related to Atoma's architecture and development:

1. Atoma Network as a decentralized permissionless protocol for AI inference.
2. Verifiable AI compute methods, including our novel sampling consensus protocol and TEEs.
3. Governance and reputation mechanisms for Atoma Network participants.
4. Node infrastructure and optimizations.
5. Applications and use cases for Atoma Network.
6. Tokenomics and economic incentives for Atoma Network participants.

