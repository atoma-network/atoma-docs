# Atoma Whitepaper

## Jorge Antonio and Hisham Khan

### Abstract

Atoma is a permissionless decentralized protocol specialized for heavy verifiable compute. Atoma derives its features from compute providers, referred to as execution nodes (or simply nodes). Nodes on the Atoma Network are responsible for processing different types of compute requests.

AI inference requests are often processed through nodes that run Large Language models (LLMs) different types of inputs (text, images, video, audio, etc) and produce some form of output (which can be either text, or other formats for multi-modality capabilities). More standard and traditional models for time series predictions, recommendation systems, etc, will also be supported on the Atoma Network.

Due to the recent advances in the field of generative AI, the demand for such compute is expected to grow exponentially in the coming years. It is estimated that decentralized cloud/edge infrastructure is expected to grow substantially by 2027, meaning it is likely that there will be substantial intersections between demand for compute, AI, and decentralized infrastructure.

Even though AI models are becoming more powerful at a fast pace, they are also becoming larger requiring higher memory and compute resources, overall. For this reason, to run the best AI models, it is crucial to have enough powerful hardware to run them at scale. For this reason, Atoma has been throughly designed to provide the right financial incentives to compute providers (also referred to as nodes) to acquire and deploy the best hardware. Such incentives combined with a robust efficient inference optimizations will lead to the best possible hardware resource utilization driving demand and lowering costs. 

That said, in order for Atoma Network to become a full intelligence layer powering Web3 protocols, it is required that AI inference is
served with high verifiable guarantees. That means, high guarantees that the AI generated output has not been tampered in any form. For this reason, we have established a novel sampling consensus protocol to ensure that Atoma Network nodes are able to provide high levels of verifiability for each AI compute request. Such sampling consensus protocol, referred to as `Sampling Consensus`, is based on the assumption that nodes are rational actors in an economic environment. Such protocol is optimized to achieve very high security guarantees, while having lower cost compared to other methods (such as zkML, opML, etc).

For the above reasons, the Atoma Network is well positioned to lead the space and opens the doors for the next generation of decentralized applications.

### Introduction

The past few years have seen the emergence of a tremendous amount of technological innovations. These include new generative AI capabilities, powered by large language models (LLMs), more powerful hardware models which allow for massive scaling computing power, and a rise of highly scalable blockchains, with lower transaction fees, through a combination of novel consensus, cryptography and modularity techniques, etc. 

To put things into perspective, it is estimate that the global market for AI is expected to grow by $100 billion by 2027, with a compound annual growth rate of 10.5%. This growth will be accompanied by a rise in the demand for compute and energy resources. Over the recent months, we have seen a rise in the awareness by a few the experts in the field around the need for governments and big tech companies to cooperate in order to build the trillion dollar compute cluster. We refer to the detailed analysis of this question by Leopold Aschenbrenner in his most recent work on [Situational Awareness](https://situational-awareness.ai/wp-content/uploads/2024/06/situationalawareness.pdf).

If such trillion dollar compute clusters will materialize in the future, is yet to be seen. What is clear, however, is the rise of demand for compute and energy resources, over the coming years.
Moreover, it is important to notice that AI pipelines require specialized hardware, such as GPUs, TPUs, etc. Even though GPUs are widely accessible to the public, most for gaming purposes, the
type of GPUs required for AI usually are of high caliber, and therefore less widely available to retail consumers and also orders of magnitude more expensive. This observation combined with the fact that there are only a few GPU manufacturers (such as NVIDIA and AMD), and that we are under geopolitical tensions that affected directly the supply chains of producing these devices, will make it difficult to make high caliber GPUs for AI accessible to retail and even small to medium size data center businesses. If this trend continues, we will step in a future in which only governments and big tech companies retain most of the world's available compute resources, while also owning the next generation AI models. Combined with the fact that such entities can have access to virtually most of the real world data, will lead to the an extreme centralization of the most revolutionary technology of our time, generative AI.

That said, Bitcoin, Ethereum and other blockchains have shown that decentralized applications can align financial incentives with massive network growth effects. These technologies provide the right framework to align incentives between compute, financial resources and permissionless communities. Inspired by the rise of decentralized applications, we are committed to build the Atoma Network, a permissionless decentralized protocol specialized for heavy verifiable AI compute. The goal of the Atoma Network is to allow compute providers to be able to monetize their compute resources in the most permissionless and transparent way possible, while being exposed to fair market pricing. In order to achieve this, we are in the process of developing Atoma node's infrastructure, which allows any compute provider (also referred to as a node) to easily deploy software that allows to execute AI compute requests, through the Atoma Network. Such infrastructure is highly optimized for best GPU utilization resources, incorporating the most recent advances in LLM memory management techniques (such as Paged Attention and vLLM), etc.

On the other hand, the Atoma Network will provide a platform for bringing demand for the usage of such compute resources themselves, working effectively as an AI market place. We plan to drive demand for compute resources at Atoma Network, by building decentralized infrastructure that provides verifiable AI compute, that is, any AI request can be followed with an attestation asserting that the AI generated output has not been tampered in any form. Such attestation can be used by the user to verify the state of the output, and can be used by the protocol to ensure that the output is legitimate. This means that Web3 protocols will be able to have access to AI, through the Atoma Network, allowing to integrate a true intelligence layer into Web3 dApps. In order to provide verifiability, we have developed a novel sampling consensus protocol, specialized in output state verifiability. Such consensus protocol is elastic, in the sense, that the end user can
decide how much of verifiability their use case requires, paying therefore a fee that is proportional to such level of verifiability. Through this, Atoma Network will be able to empower Web3 applications, as well as naturally centralized Web2 applications. 

That said, there will be demand for Atoma Network functionality beyond smart contracts, directly. Applications such as UI chat applications, AI Web3 wallet integrations, online data scrapping,
next generation browsers and browser extensions, community based knowledge bases, social media platforms, etc. Such applications will not require the same level of trust assumptions as general smart contracts, but will still require certain integrity guarantees. As an example, a DeFi protocol may require almost 100% certainty that a given AI market liquidity prediction was computed correctly, even a small deviation from the original prediction can have a huge impact on the protocol's future funds management. However, a user interacting with an online community based chat application is mostly interested in knowing that it paid the right fee to have access to the right model for output generation, and not a less capable model. Therefore, our sampling consensus is adapted to each of such use cases, being therefore highly flexible and optimizing for a wide range of different use cases. We are currently exploring other verifiability techniques, such as Trusted Execution Environments (TEEs), that will allow for even higher levels of verifiability, while also reducing the cost of verifiable execution close to native cost.

The Atoma Network will be empowered by its TOMA token. Through a robust tokenomics, both compute providers, protocol participants and developers will be rewarded by their contribution to maintaining the network longevity. The TOMA token will be required to pay for AI inference fees, for nodes to deposit collateral to be able to participate on the network, and also for developers to being paid for their contributions to the Atoma Network. Moreover, the tokenomics follow an initial period of inflation in order to subsidize the network's growth. This growth includes both incentivizing node operators to acquire better compute hardware, to incentivize model creators to develop new AI models, specialized for applications of interest to the Atoma Network (such as financial market predictions, etc), as well as to incentivize developers to build applications that drive demand for the available compute resources on the Atoma Network. After the network's inflationary period (which will last a few years), the tokenomics will follow a more balanced dynamics, through tail emissions and burning mechanisms. Together, both mechanisms will lead to a gradual decrease in the total supply of the TOMA token, contributing for its increase in value provided the demand for Atoma Network increases, while also allowing for a some new allocation of the token to the protocol's treasury and to value providers (in the form of new node providers, model creators, developers, etc).

In order to empower the next generation applications in the intersection of Web3 and AI, both data and model privacy is of utmost importance. Through the use of TEEs, we will not only be able to provide verifiable AI compute, but also provide full data privacy to the end user. This means that Atoma nodes will not be able to access any sensitive data, such as personal information, while also not being able to access the output generated by the AI model. Only the end user will be able to decrypt the generated output, and can have high assurance that the input to the model was not shared with any third party. Through this, users can have full control of their data, allowing for AI models to operate on sensitive data. Privacy will unlock the full potential of AI, allowing for the creation of new data economic revenue models, as well applications that can operate on the user's behalf without revealing anything about the user. 

On another note, the development of new AI models will be crucial for the future of the Atoma Network. We believe that open source AI development will be crucial to democratize and broaden the reach of AI technologies across societies. We are committed to incentivize the open source AI community to develop and deploy new AI models, while being able to monetize their work, allowing for experts in the community to earn a continuous stream of revenue through their model utilization.

A decentralized platform such as Atoma Network will put forth combined efforts that will ultimately lead to a new era of use cases within decentralized applications, driving financial incentives for computing power to be partially decouple from centralized cloud providers. Smart contracts will not only deploy new financial products, but also be able to deploy and orchestrate compute resources, powering new decentralized protocols. 

The focus of the current text is to explore at a deeper level the following topics related to Atoma's architecture and development:

1. Atoma Network as a decentralized permissionless protocol for AI inference.
2. Verifiable AI compute methods, including our novel sampling consensus protocol and TEEs.
3. Governance and reputation mechanisms for Atoma Network participants.
4. Node infrastructure and optimizations.
5. Applications and use cases for Atoma Network.
6. Tokenomics and economic incentives for Atoma Network participants.

