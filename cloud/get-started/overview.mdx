---
title: 'Overview'
description: 'Overview of Atoma Proxy'
---

## What is Atoma Proxy?

The Atoma Network is a decentralized and permissionless network of nodes that provide computing resources to AI workloads. In particular, this means that Atoma is composed of a large number of active nodes
monetizing their hardware resources. These nodes vary in capabilities, geographic locations, bandwidth, and latency specifications.

Even though this heterogeneity allows for a more flexible and resilient infrastructure with better price discovery for compute, it is a challenge to manage and efficiently distribute the AI workloads across the network.
On top of that, Atoma nodes are only required to process properly authenticated and authorized requests. This implies that on-chain payments need to be due in advance for each processed request, including proof of ownership
of funds. This process involves complex programmable cryptographic primitives, which can become a challenge for the typical AI developer wishing to use Atoma's infrastructure.

In order to overcome these challenges, we have introduced the concept of the **Atoma Proxy**. The Atoma Proxy is a vital component of the Atoma Network, acting as an intelligent middleman for AI workloads, efficiently managing and distributing tasks across a network of computers.
Furthermore, the Atoma Proxy is designed to streamline both user and developer experiences by offering
a fully compatible OpenAI API. It abstracts the complexity of the underlying infrastructure, including handling payments, request authentication and authorization, and intelligently routing requests to the most suitable available nodes.

For an AI developer willing to access LLM inference through Atoma, it can simply interact with the Atoma Proxy as if it was interacting via the Proxy's OpenAI API. Moreover, the Atoma Proxy supports both stablecoin and native USD payments,
ensuring a seamless experience for developers. This means developers can build applications with minimal overhead while fully leveraging the power of the Atoma Network.

That said, we prioritize not only the developer experience but also the principles of decentralized technologies. To uphold this, we have open-sourced the Atoma Proxy codebase, allowing anyone to run their own instance of the Atoma Proxy and fully leverage the power of the Atoma Network without depending on our
native proxy infrastructure.

Finally, the Atoma Proxy preserves the integrity and confidentiality of the data processed by the Atoma Network, by supporting confidential computing, for both private and verifiable AI compute.
This means, that the data processed by the Atoma Network is protected from unauthorized access, including the Atoma Proxy and the Atoma nodes.

## Interact with Atoma's Proxy Infrastructure

We invite you to explore our [Atoma cloud dashboard](https://cloud.atoma.network), where you can create an account and generate an API key. With this API key, you can interact with Atoma's Proxy infrastructure, and leverage the full power of the Atoma Network.
Furthermore, you can also deploy your own instance of the Atoma Proxy on your own infrastructure, by consulting our open-source codebase [here](https://github.com/atoma-network/atoma-proxy).

Finally, you can make direct API calls to the Atoma Proxy, by using the following endpoints:

- Chat completions: `https://api.atoma.network/v1/chat/completions`
- Confidential chat completions: `https://api.atoma.network/v1/confidential/chat/completions/`
- Embeddings: `https://api.atoma.network/v1/embeddings`
- Confidential embeddings: `https://api.atoma.network/v1/confidential/embeddings/`
- Images generations: `https://api.atoma.network/v1/images/generations`
- Confidential images generations: `https://api.atoma.network/v1/confidential/images/generations/`
- Consult available models: `https://api.atoma.network/v1/models`

You can find more information about the API itself in the [API reference](https://docs.atoma.network/cloud-api-reference/).

## Key Features

### 1. OpenAI Compatibility
- Drop-in replacement for OpenAI API
- Supports standard endpoints:
  - `/v1/chat/completions`
  - `/v1/embeddings` 
  - `/v1/images/generations`
  - `/v1/models`

### 2. Load Balancing
- Distributes AI workloads evenly across different computers
- Prevents any single machine from getting overwhelmed
- Ensures efficient resource usage

### 3. Smart Routing
- Directs AI requests to the most suitable computers
- Considers factors like:
  - Model availability
  - Current workload
  - Performance capabilities

### 4. High Availability
- Keeps the service running smoothly
- Has backup plans if something goes wrong
- Prevents service interruptions

### 5. Security
- Implements authentication for API access
- Supports confidential computing, for both private and verifiable AI compute.
- Protects sensitive data during processing

## How It Works

1. **Request Reception**
   - Receives AI task requests using OpenAI-compatible endpoints
   - Validates the requests and authentication

2. **Task Distribution**
   - Analyzes the network's current state
   - Picks the best computer to handle the task
   - Routes the request to the chosen computer

3. **Monitoring**
   - Keeps track of all ongoing tasks
   - Monitors system health
   - Collects performance metrics

## Getting Started

The easiest way to run Atoma Proxy is using Docker:
